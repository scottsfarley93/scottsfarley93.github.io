<h1 id="introduction">Introduction</h1>

<p>21st century climate change is expected to significantly alter species
distributions, both at global and regional scales. Species Distribution
Models (SDMs) are statistical methods that estimate species-specific
responses to climatic gradients, and are widely used to to predict
species presence under future climate scenarios [@franklin2010]. While
the models are widespread in the literature, a thorough understanding of
algorithm execution time and accuracy produced by different input
datasets and on different computing hardware has not yet been
established. Here we discuss models for predicting the accuracy and run
time of three SDMs given these factors. Execution time and accuracy
models can improve computing resource utilization and identify
performance bottlenecks in popular SDM code repositories.</p>

<p>SDMs can be fit with both modern- and paleo- climate training data, the
scale, size, and resolution of which has increased rapidly over the last
several years. Emerging databases, such as the Neotoma Paleoecological
Database (http://neotomadb.org) and the Global Biodiversity Information
Facility (GBIF, http://gbif.org), provide biogeographical data for
millions of species worldwide, both in the recent fossil record and for
the modern era. Environmental covariates to species presences are
obtained from widely-available climate model output, which can provide
decadal or sub-decadal temporal resolution for the last 21,000 years.
Downscaling techniques can improve the spatial resolution of gridded
data to scales suitable for regional and sub-regional study.</p>

<p>While the size and resolution of climatic and biodiversity data used to
train SDMs increase, the methods used to learn species’ responses to
climatic gradients are becoming more computationally expensive. Most
competitive SDMs use statistical learning procedures to estimate the
functional relationship between species presence and climatic patterns.
Novel techniques, such as Bayesian learning have also demonstrated high
accuracy in this setting [@MEE3:MEE312523]. Moreover, many researchers
now model hundreds of species in a single study (e.g.,
[@elith2006novel]), or use joint modeling techniques to capture
inter-species interactions [@clark2014more], resulting in larger
modeling workloads. More powerful computing hardware has the potential
to reduce the execution time of SDMs, particularly those with high
dimensionality, large training sets, and/or wide spatiotemporal extents.
While work has been done to assess the characteristic complexity of
machine learning models (e.g. big-O notation) [@hastie2001additive],
less has been done to characterize the differences in model execution
time of SDM techniques due to different computing hardware
configurations and algorithm inputs. Though internal variations in
memory management make it is difficult to exactly define model runtime
as a function of hardware [@lilja2005measuring], models of computer
performance that consider input data and static hardware configuration
may be capable of capturing high-level trends [@wu2011performance],
[@lee2007methods].</p>

<p>Here we model algorithm speed using two static hardware components
capable of improving performance: (1) main memory size (i.e., RAM) and
(2) the number computing cores, and two algorithm inputs: (1) the size
of the training data used to fit the model and (2) the spatial
resolution of the output. While different learning techniques may have
implementation- or algorithm-specific differences (i.e., tuning
parameters, language differences) that may influence model execution
time, we test several popular $R$ implementations with experimental
variables that extend across model classes.</p>

<p>We also examine the predictive accuracy gains made by fitting SDMs with
different training data sizes. Recent studies have examined the best
practices for using small numbers of training examples ($n &lt; 300$), such
as for rare species [@wisz2008effects]. However, while very large
training datasets ($n &gt; 100,000$) are unlikely in the ecological domain,
the fossil record can be used to fit SDMs with a larger set of training
data than the modern era alone [@maguire2015modeling], perhaps by
several times for some species. While there is a greater degree of
uncertainty associated with fossil occurrences, their utilization may
significantly enhance SDM skill when included in the fitting process.</p>

<h1 id="method">Method</h1>

<p>We systematically tested the accuracy and execution time of three
popular species distribution modeling algorithms on four different
training set sizes and four spatial resolutions on 44 computing
configurations (4 x CPU, 11 x RAM). All experiments were done using the
R programming language on virtual machines hosted on the Google Cloud
Compute platform. Ten replicates of each combination of hardware and
algorithm inputs were completed to improve understanding system-induced
variance.</p>

<p>Fossil occurrences for the <em>Picea</em> (spruce) genus over the last 21,000
years were obtained from the Neotoma Paleoecological Database
(http://neotomadb.org). Decadally-averaged climatic covariates for each
fossil occurrence were extracted from 0.5 degree spatial resolution
debiased and downscaled CCSM3 climate model output for North America
[@dryad_1597g], and used to fit the SDMs.</p>

<p>Three SDM algorithms that have shown competitive predictive skill in the
literature were evaluated: (a) boosted regression trees (GBM-BRT)
[@elith2008working], (b) multivariate adaptive regression splines (MARS)
[@leathwick2006comparative], (c) generalized additive models (GAM)
[@guisan2002generalized]. All models were fit using a randomized
training data subset of a pre-specified size, and then projected onto a
climatic grid for the year 2100. The output grid resolution was varied
between 0.1 and 1 degree of latitude. SDM accuracy skill was evaluated
using an independent testing set of 20% of the available data using the
area under the receiver operator curve (AUC) statistic.</p>

<p>To predict model execution time, two predictive models were built for
each SDM technique: (a) a multiple linear regression and (b) a gradient
boosted regression tree model. Models were fit from the set of
experiments ($n=20,583$) using the number of training examples, CPU
cores, memory, and spatial resolution as predictors of execution time.
Model were evaluated using ANOVA and partial dependence plots, and skill
was estimated using observed-to-predicted correlation metrics.
Predictive models of SDM AUC score were also developed and fit using a
boosted regression tree approach.</p>

<h1 id="evaluation">Evaluation</h1>

<p>=0.95</p>

<p>Predictive models of SDM execution time demonstrate considerable skill
when evaluated against a holdout testing set of observed values. In
general, the boosted regression tree model approach significantly
outperformed the linear models. Regression trees are able better capture
the potential non-linearities of the experimental dataset and can remove
the negative predictions forecast by the linear model. However, both
sets of models consistently showed $r^2 &gt; 0.8$ correlation between
observed and predicted values with a mean prediction error of less than
4 seconds.</p>

<p>Of all six execution time models (2 models x 3 SDMs), the regression
tree prediction model of the MARS SDM performed the best, with a mean
error of $-0.457 \pm 1.895$ seconds and an $r^2$ value of 0.936. The
regression tree models for GAM and GBM-BRT SDMs both performed well with
r^2^ values of 0.892 and 0.880, respectively. The linear models all
showed lower r^2^ correlation values and had larger prediction variance
and mean prediction errors than their decision tree counterparts. The
best performing linear model was again for the MARS SDM, with an r^2^
correlation of 0.876, with a significantly larger mean error of
$2.17 \pm 1.73$ seconds. Figure [fig:timeFig] shows the observed and
predicted values for each SDM for both of the prediction models.</p>

<p>Model interrogation using ANOVA (linear model) and partial dependency
plots (GBM model) reveals that model execution time depends strongly on
the number of training examples used to fit the SDM. In all cases, the
number of training examples and spatial resolution of the output were
shown to be highly significant ($p &lt; 0.001$). Computer hardware
variables were not shown to be significant predictors of execution time
for these SDMs. In some cases, additional memory was shown to reduce
model speed, perhaps due to increased overhead of memory management.
Runtime logs indicate that model execution was bounded by CPU processing
capability, rather than main memory capacity, suggesting that SDM
workflows could be improved if the algorithms were written to run in
parallel, rather than sequentially.</p>

<p>Models of SDM accuracy suggest that significant accuracy gains can be
achieved by fitting the models with more than 2000 training examples.
All three SDM algorithms showed a similar pattern of increasing accuracy
as the number of input training examples increased, though the increase
was not linear. Figure [fig:accFig] demonstrates the accuracy of a
GBM-BRT model with up to 9000 training training examples. The accuracy
prediction model shows an observed-to-predicted $r^2$ of 0.900 and a
mean prediction error of $0.001 \pm 0.002$ AUC. The model strongly
suggests that use of the additional training data available in the
fossil record can significantly enhance SDM accuracy.</p>

<p>=0.95</p>

<p>Future work will be directed towards larger and more complex models of
climate-species dynamics. Additional research should also investigate
explicitly parallel machine learning techniques and their feasibility
for SDM studies, as our results show that execution time is strongly
limited by CPU-bound serial learning techniques.</p>

<h1 id="acknowledgments-acknowledgments-unnumbered">Acknowledgments {#acknowledgments .unnumbered}</h1>

<p>Funding for the authors was provided by University of Wisconsin-Madison
Geography Department’s Trewartha Research Award, the University of
Wisconsin-Madison Vilas Research Trust, and the National Science
Foundation ( EAR-1550707).</p>

